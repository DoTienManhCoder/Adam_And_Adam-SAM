# So s√°nh Thu·∫≠t to√°n T·ªëi ∆∞u: Adam vs Adam+SAM

D·ª± √°n n√†y th·ª±c hi·ªán **3 th·ª±c nghi·ªám c∆° b·∫£n** v√† **2 th·ª±c nghi·ªám b·ªï sung** ƒë·ªÉ so s√°nh to√†n di·ªán hi·ªáu su·∫•t c·ªßa thu·∫≠t to√°n Adam v·ªõi Adam k·∫øt h·ª£p Sharpness-Aware Minimization (SAM).

## üìã M·ª•c l·ª•c

### Th·ª±c nghi·ªám
1. [Th·ª±c nghi·ªám 1: Logistic Regression tr√™n MNIST](#-th·ª±c-nghi·ªám-1-logistic-regression-tr√™n-mnist)
2. [Th·ª±c nghi·ªám 2: MLP tr√™n MNIST](#-th·ª±c-nghi·ªám-2-mlp-tr√™n-mnist)
3. [Th·ª±c nghi·ªám 3: CNN nh·ªè tr√™n CIFAR-10](#-th·ª±c-nghi·ªám-3-cnn-nh·ªè-tr√™n-cifar-10)
4. [Th·ª±c nghi·ªám b·ªï sung 1: High Learning Rate](#-th·ª±c-nghi·ªám-b·ªï-sung-1-high-learning-rate)
5. [Th·ª±c nghi·ªám b·ªï sung 2: Small Data Regime](#-th·ª±c-nghi·ªám-b·ªï-sung-2-small-data-regime-√≠t-d·ªØ-li·ªáu)

### Ph√¢n t√≠ch v√† K·∫øt lu·∫≠n
- [B√°o c√°o t·ªïng h·ª£p](#-b√°o-c√°o-t·ªïng-h·ª£p)
  - [M·ª•c ƒë√≠ch th·ª±c nghi·ªám](#1-m·ª•c-ƒë√≠ch-th·ª±c-nghi·ªám-t·ªïng-quan)
  - [K·∫øt qu·∫£ th·ª±c nghi·ªám](#2-k·∫øt-qu·∫£-th·ª±c-nghi·ªám-t·ªïng-h·ª£p)
  - [ƒê√°nh gi√°](#3-ƒë√°nh-gi√°-v√†-so-s√°nh)
  - [K·∫øt lu·∫≠n](#4-k·∫øt-lu·∫≠n)
  - [H∆∞·ªõng ph√°t tri·ªÉn](#5-h∆∞·ªõng-ph√°t-tri·ªÉn)

## üöÄ C√†i ƒë·∫∑t

### Y√™u c·∫ßu h·ªá th·ªëng
- Python 3.8 tr·ªü l√™n
- GPU NVIDIA v·ªõi CUDA support (khuy·∫øn ngh·ªã ƒë·ªÉ tƒÉng t·ªëc ƒë√°ng k·ªÉ)
- 4GB+ RAM (8GB+ khuy·∫øn ngh·ªã cho CNN)

### ‚ö†Ô∏è L∆ØU √ù QUAN TR·ªåNG V·ªÄ GPU

**V·∫•n ƒë·ªÅ**: N·∫øu b·∫°n c√≥ GPU NVIDIA nh∆∞ng code v·∫´n ch·∫°y tr√™n CPU, nguy√™n nh√¢n l√† b·∫°n ƒë√£ c√†i ƒë·∫∑t **PyTorch phi√™n b·∫£n CPU** thay v√¨ phi√™n b·∫£n CUDA.

**Ki·ªÉm tra GPU**:
```bash
# Ki·ªÉm tra xem PyTorch c√≥ nh·∫≠n GPU kh√¥ng
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"
```

N·∫øu hi·ªÉn th·ªã `CUDA available: False`, b·∫°n c·∫ßn c√†i ƒë·∫∑t l·∫°i PyTorch v·ªõi CUDA support.

### C√†i ƒë·∫∑t th∆∞ vi·ªán

#### B∆∞·ªõc 1: X√°c ƒë·ªãnh phi√™n b·∫£n CUDA c·ªßa GPU
```bash
nvidia-smi
```
L·ªánh n√†y s·∫Ω hi·ªÉn th·ªã phi√™n b·∫£n CUDA (v√≠ d·ª•: CUDA 12.8, 12.4, 11.8...)

#### B∆∞·ªõc 2: G·ª° c√†i ƒë·∫∑t PyTorch CPU (n·∫øu ƒë√£ c√†i)
```bash
pip uninstall torch torchvision torchaudio -y
```

#### B∆∞·ªõc 3: C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support

**Cho CUDA 12.x** (RTX 30xx, 40xx, A100...):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

**Cho CUDA 11.8** (GTX 16xx, RTX 20xx...):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c√≤n l·∫°i**:
```bash
pip install matplotlib numpy
```

**Ho·∫∑c d√πng requirements.txt** (sau khi ƒë√£ c√†i PyTorch CUDA):
```bash
pip install -r requirements.txt
```

### Ki·ªÉm tra c√†i ƒë·∫∑t th√†nh c√¥ng
Sau khi c√†i ƒë·∫∑t, ch·∫°y l·ªánh n√†y ƒë·ªÉ x√°c nh·∫≠n GPU ho·∫°t ƒë·ªông:
```bash
python check_gpu.py
```

K·∫øt qu·∫£ mong ƒë·ª£i:
```
CUDA available: True
GPU name: NVIDIA GeForce RTX xxxx
```

---

## üìä Th·ª±c nghi·ªám 1: Logistic Regression tr√™n MNIST

### M√¥ t·∫£
- **M√¥ h√¨nh**: Logistic Regression (Linear layer ƒë∆°n gi·∫£n)
- **Dataset**: MNIST (28x28 grayscale images, 10 classes)
- **S·ªë tham s·ªë**: ~7,850
- **Epochs**: 50
- **Batch size**: 128
- **Learning rate**: 0.001
- **Optimizer**: Adam / Adam+SAM (rho=0.05)

### Ch·∫°y th·ª±c nghi·ªám

**N·∫øu s·ª≠ d·ª•ng Virtual Environment** (.venv):
```bash
# K√≠ch ho·∫°t virtual environment tr∆∞·ªõc
.\.venv\Scripts\Activate.ps1  # Windows PowerShell
# ho·∫∑c
.venv\Scripts\activate.bat     # Windows CMD

# Sau ƒë√≥ ch·∫°y
cd "Logistic Regression tr√™n MNIST"
python logistic_regression_mnist.py
```

**Ho·∫∑c d√πng ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß**:
```bash
cd "Logistic Regression tr√™n MNIST"
C:/Users/<YourUsername>/Documents/GitHub/Adam_And_Adam-SAM/.venv/Scripts/python.exe logistic_regression_mnist.py
```

**N·∫øu kh√¥ng d√πng Virtual Environment**:
```bash
cd "Logistic Regression tr√™n MNIST"
python logistic_regression_mnist.py
```

---

## üìä Th·ª±c nghi·ªám 2: MLP tr√™n MNIST

### M√¥ t·∫£
- **M√¥ h√¨nh**: Multi-Layer Perceptron (2 hidden layers: 256, 128)
- **Dataset**: MNIST (28x28 grayscale images, 10 classes)
- **S·ªë tham s·ªë**: ~235,146
- **Epochs**: 50
- **Batch size**: 128
- **Learning rate**: 0.001
- **Dropout**: 0.2
- **Optimizer**: Adam / Adam+SAM (rho=0.05)

### Ch·∫°y th·ª±c nghi·ªám

**N·∫øu s·ª≠ d·ª•ng Virtual Environment** (.venv):
```bash
# K√≠ch ho·∫°t virtual environment tr∆∞·ªõc
.\.venv\Scripts\Activate.ps1  # Windows PowerShell

# Sau ƒë√≥ ch·∫°y
cd "MLP tr√™n MNIST"
python mlp_mnist.py
```

**Ho·∫∑c d√πng ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn Python trong venv**:
```bash
cd "MLP tr√™n MNIST"
C:/Users/<YourUsername>/Documents/GitHub/Adam_And_Adam-SAM/.venv/Scripts/python.exe mlp_mnist.py
```

---

## üìä Th·ª±c nghi·ªám 3: CNN nh·ªè tr√™n CIFAR-10

### M√¥ t·∫£
- **M√¥ h√¨nh**: Small CNN (3 conv layers + 2 FC layers)
- **Dataset**: CIFAR-10 (32x32 color images, 10 classes)
- **S·ªë tham s·ªë**: ~588,042
- **Epochs**: 100
- **Batch size**: 128
- **Learning rate**: 0.001
- **Data augmentation**: Random crop, horizontal flip
- **Optimizer**: Adam / Adam+SAM (rho=0.05)

‚ö†Ô∏è **L∆∞u √Ω:** Th·ª±c nghi·ªám n√†y m·∫•t nhi·ªÅu th·ªùi gian h∆°n (100 epochs): ~10-15 ph√∫t tr√™n GPU, ~2-3 gi·ªù tr√™n CPU.

### Ch·∫°y th·ª±c nghi·ªám

**N·∫øu s·ª≠ d·ª•ng Virtual Environment** (.venv):
```bash
# K√≠ch ho·∫°t virtual environment tr∆∞·ªõc
.\.venv\Scripts\Activate.ps1  # Windows PowerShell

# Sau ƒë√≥ ch·∫°y
cd "CNN tr√™n CIFAR-10"
python cnn_cifar10.py
```

**Ho·∫∑c d√πng ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn Python trong venv**:
```bash
cd "CNN tr√™n CIFAR-10"
C:/Users/<YourUsername>/Documents/GitHub/Adam_And_Adam-SAM/.venv/Scripts/python.exe cnn_cifar10.py
```

---

## üî¨ Th·ª±c nghi·ªám b·ªï sung 1: High Learning Rate

### M√¥ t·∫£
- **M√¥ h√¨nh**: ResNet-18 (modified cho CIFAR-10)
- **Dataset**: CIFAR-10 (50,000 train, 10,000 test)
- **Learning Rates th·ª≠ nghi·ªám**: 0.001, 0.005, 0.01
- **Epochs**: 50
- **Batch size**: 128

**M·ª•c ƒë√≠ch**: Ki·ªÉm tra ƒë·ªô ·ªïn ƒë·ªãnh c·ªßa SAM v·ªõi learning rate cao - t√¨nh hu·ªëng Adam th∆∞·ªùng g·∫∑p kh√≥ khƒÉn.

### Ch·∫°y th·ª±c nghi·ªám

```bash
cd "Th·ª±c nghi·ªám b·ªï sung"

# K√≠ch ho·∫°t venv
..\.venv\Scripts\Activate.ps1

# Ch·∫°y (m·∫•t ~3-4 gi·ªù tr√™n GPU)
python high_lr_experiment.py
```

‚ö†Ô∏è **L∆∞u √Ω:** Th·ª±c nghi·ªám n√†y train 6 models (3 LR √ó 2 optimizers) n√™n m·∫•t nhi·ªÅu th·ªùi gian.

---

## üìä Th·ª±c nghi·ªám b·ªï sung 2: Small Data Regime (√çt D·ªØ Li·ªáu)

### M√¥ t·∫£
- **M√¥ h√¨nh**: ResNet-18 (modified cho CIFAR-10)
- **Dataset**: CIFAR-10 v·ªõi **ch·ªâ 10% training data** (5,000 samples thay v√¨ 50,000)
- **Test set**: Gi·ªØ nguy√™n 10,000 samples
- **Epochs**: 100
- **Learning rate**: 0.001
- **Batch size**: 64 (gi·∫£m do data √≠t)

**M·ª•c ƒë√≠ch**: Ki·ªÉm tra kh·∫£ nƒÉng ch·ªëng overfitting c·ªßa SAM khi d·ªØ li·ªáu training r·∫•t h·∫°n ch·∫ø.

### Ch·∫°y th·ª±c nghi·ªám

```bash
cd "Th·ª±c nghi·ªám b·ªï sung"

# K√≠ch ho·∫°t venv
..\.venv\Scripts\Activate.ps1

# Ch·∫°y (m·∫•t ~2-3 gi·ªù tr√™n GPU)
python small_data_experiment.py
```

‚ö†Ô∏è **L∆∞u √Ω:** M·∫∑c d√π data √≠t h∆°n nh∆∞ng train 100 epochs n√™n v·∫´n m·∫•t nhi·ªÅu th·ªùi gian.

---

## üìà K·∫øt qu·∫£ v√† Bi·ªÉu ƒë·ªì

M·ªói th·ª±c nghi·ªám s·∫Ω t·ª± ƒë·ªông:
1. T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu
2. Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi Adam
3. Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi Adam+SAM
4. T·∫°o bi·ªÉu ƒë·ªì so s√°nh (l∆∞u d∆∞·ªõi d·∫°ng PNG)
5. In k·∫øt qu·∫£ chi ti·∫øt ra console

### C√°c bi·ªÉu ƒë·ªì ƒë∆∞·ª£c t·∫°o ra:

**Th·ª±c nghi·ªám c∆° b·∫£n:**
- `logistic_regression_comparison.png` - Th·ª±c nghi·ªám 1: Logistic Regression tr√™n MNIST
- `mlp_comparison.png` - Th·ª±c nghi·ªám 2: MLP tr√™n MNIST
- `cnn_cifar10_comparison.png` - Th·ª±c nghi·ªám 3: CNN tr√™n CIFAR-10

**Th·ª±c nghi·ªám b·ªï sung:**
- `high_lr_comparison.png` - Th·ª±c nghi·ªám 4: So s√°nh v·ªõi learning rate kh√°c nhau
- `small_data_comparison.png` - Th·ª±c nghi·ªám 5: So s√°nh v·ªõi √≠t d·ªØ li·ªáu

M·ªói bi·ªÉu ƒë·ªì bao g·ªìm 4 subplot:
- Training Loss
- Test Loss
- Training Accuracy
- Test Accuracy

**ƒê·∫∑c bi·ªát:** Bi·ªÉu ƒë·ªì th·ª±c nghi·ªám b·ªï sung c√≥ nhi·ªÅu ƒë∆∞·ªùng (multiple learning rates ho·∫∑c data sizes) ƒë·ªÉ so s√°nh r√µ h∆°n.

---

## üìä B√°o c√°o t·ªïng h·ª£p

### 1. M·ª•c ƒë√≠ch th·ª±c nghi·ªám (T·ªïng quan)

D·ª± √°n n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ **ƒë√°nh gi√° to√†n di·ªán** thu·∫≠t to√°n SAM (Sharpness-Aware Minimization) so v·ªõi Adam optimizer truy·ªÅn th·ªëng qua nhi·ªÅu g√≥c ƒë·ªô v√† ƒëi·ªÅu ki·ªán kh√°c nhau:

#### C√¢u h·ªèi nghi√™n c·ª©u ch√≠nh:

1. **SAM c√≥ th·ª±c s·ª± c·∫£i thi·ªán kh·∫£ nƒÉng t·ªïng qu√°t h√≥a kh√¥ng?**
   - So s√°nh test accuracy gi·ªØa Adam v√† Adam+SAM
   - ƒêo l∆∞·ªùng m·ª©c ƒë·ªô gi·∫£m overfitting

2. **SAM ho·∫°t ƒë·ªông th·∫ø n√†o tr√™n c√°c m√¥ h√¨nh kh√°c nhau?**
   - T·ª´ ƒë∆°n gi·∫£n (Logistic Regression) ƒë·∫øn ph·ª©c t·∫°p (CNN, ResNet)
   - T·ª´ √≠t parameters (~8K) ƒë·∫øn nhi·ªÅu parameters (~600K+)

3. **SAM c√≥ gi√° tr·ªã trong c√°c t√¨nh hu·ªëng th·ª±c t·∫ø kh√¥ng?**
   - Khi thi·∫øu d·ªØ li·ªáu training (common trong medical imaging, specialized domains)
   - Khi c·∫ßn training nhanh v·ªõi learning rate cao
   - Khi dataset kh√≥ v√† d·ªÖ overfit

4. **Trade-off c√≥ ƒë√°ng gi√° kh√¥ng?**
   - Chi ph√≠ t√≠nh to√°n tƒÉng 2x
   - C·∫£i thi·ªán accuracy bao nhi√™u %
   - Khi n√†o n√™n d√πng SAM, khi n√†o n√™n d√πng Adam

#### Ph∆∞∆°ng ph√°p nghi√™n c·ª©u:

**Th·ª±c nghi·ªám c∆° b·∫£n (3 th√≠ nghi·ªám):**
- **Th·ª±c nghi·ªám 1 - Logistic Regression tr√™n MNIST:** Baseline ƒë∆°n gi·∫£n nh·∫•t, ki·ªÉm tra SAM tr√™n linear model
- **Th·ª±c nghi·ªám 2 - MLP tr√™n MNIST:** Th√™m depth v√† complexity, test v·ªõi dropout regularization
- **Th·ª±c nghi·ªám 3 - CNN tr√™n CIFAR-10:** B√†i to√°n th·ª±c t·∫ø kh√≥ h∆°n, model ph·ª©c t·∫°p h∆°n, training l√¢u h∆°n

**Th·ª±c nghi·ªám b·ªï sung (2 th√≠ nghi·ªám - ƒëi·ªÅu ki·ªán extreme):**
- **Th·ª±c nghi·ªám 4 - High Learning Rate:** Test robustness, xem SAM c√≥ ·ªïn ƒë·ªãnh h∆°n Adam khi LR cao
- **Th·ª±c nghi·ªám 5 - Small Data:** Test generalization, xem SAM c√≥ ch·ªëng overfit t·ªët h∆°n khi data √≠t

#### Metrics ƒë√°nh gi√°:

1. **Test Accuracy:** ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p test (ch·ªâ s·ªë ch√≠nh)
2. **Overfitting Gap:** Train Acc - Test Acc (ƒëo m·ª©c ƒë·ªô overfit)
3. **Training Time:** Th·ªùi gian training (ƒëo chi ph√≠ t√≠nh to√°n)
4. **Learning Curve Stability:** ƒê·ªô m∆∞·ª£t m√† c·ªßa loss/accuracy curves
5. **Best vs Final Test Acc:** Xem model c√≥ maintain performance hay gi·∫£m v·ªÅ cu·ªëi

---

### 2. K·∫øt qu·∫£ th·ª±c nghi·ªám (T·ªïng h·ª£p)

#### B·∫£ng t·ªïng h·ª£p to√†n b·ªô 5 th·ª±c nghi·ªám:

| Th·ª±c nghi·ªám | Dataset | Model | ƒêi·ªÅu ki·ªán | Adam Test Acc | SAM Test Acc | C·∫£i thi·ªán | Overfitting Gap (Adam‚ÜíSAM) | Training Time Ratio |
|-------------|---------|-------|-----------|---------------|--------------|-----------|---------------------------|---------------------|
| **1. Logistic Regression** | MNIST | Linear | Standard | 92.5% | 93.6% | **+1.1%** | 0.7% ‚Üí 0.5% (-29%) | 2x |
| **2. MLP** | MNIST | 2-layer | Standard | 97.8% | 98.4% | **+0.6%** | 1.5% ‚Üí 0.3% (-80%) | 2x |
| **3. CNN** | CIFAR-10 | 3 conv + 2 FC | Standard | 77.3% | 79.8% | **+2.5%** | 14.4% ‚Üí 9.1% (-37%) | 1.5x |
| **4. High LR (LR=0.01)** | CIFAR-10 | ResNet-18 | LR cao | 52.1% | 74.6% | **+22.5%** | N/A (Adam diverge) | 2x |
| **5. Small Data** | CIFAR-10 | ResNet-18 | 10% data | 58.3% | 67.8% | **+9.5%** | 37.9% ‚Üí 19.6% (-48%) | 2x |

#### Ph√¢n t√≠ch chi ti·∫øt theo t·ª´ng th·ª±c nghi·ªám:

##### **Th·ª±c nghi·ªám 1: Logistic Regression tr√™n MNIST**

**K·∫øt qu·∫£:**
- Adam: Train 93.2%, Test 92.5%
- SAM: Train 94.1%, Test 93.6%
- C·∫£i thi·ªán: +1.1% test accuracy

**Quan s√°t:**
- SAM c·∫£i thi·ªán nh·∫π ngay c·∫£ tr√™n model ƒë∆°n gi·∫£n nh·∫•t
- Overfitting gap gi·∫£m t·ª´ 0.7% xu·ªëng 0.5%
- Training m∆∞·ª£t m√† h∆°n, √≠t fluctuation
- Chi ph√≠ 2x th·ªùi gian nh∆∞ng ch·∫•p nh·∫≠n ƒë∆∞·ª£c do model nh·ªè

**√ù nghƒ©a:** Ch·ª©ng minh SAM ho·∫°t ƒë·ªông ngay c·∫£ tr√™n linear model, nh∆∞ng l·ª£i √≠ch ch∆∞a n·ªïi b·∫≠t.

---

##### **Th·ª±c nghi·ªám 2: MLP tr√™n MNIST**

**K·∫øt qu·∫£:**
- Adam: Train 99.3%, Test 97.8%, Gap 1.5%
- SAM: Train 98.7%, Test 98.4%, Gap 0.3%
- C·∫£i thi·ªán: +0.6% test accuracy, overfitting gap gi·∫£m 80%

**Quan s√°t:**
- **SAM b·∫Øt ƒë·∫ßu t·ªèa s√°ng:** Overfitting gap gi·∫£m m·∫°nh (1.5% ‚Üí 0.3%)
- Train accuracy th·∫•p h∆°n nh∆∞ng test accuracy cao h∆°n ‚Üí generalize t·ªët
- K·∫øt h·ª£p t·ªët v·ªõi Dropout regularization
- Learning curve ·ªïn ƒë·ªãnh h∆°n, √≠t spikes

**√ù nghƒ©a:** SAM th·ª±c s·ª± hi·ªáu qu·∫£ khi model c√≥ depth. Flat minima b·∫Øt ƒë·∫ßu th·ªÉ hi·ªán gi√° tr·ªã.

---

##### **Th·ª±c nghi·ªám 3: CNN tr√™n CIFAR-10**

**K·∫øt qu·∫£:**
- Adam: Best Test 77.3%, Final 76.8%, Gap 14.4%
- SAM: Best Test 79.8%, Final 79.5%, Gap 9.1%
- C·∫£i thi·ªán: +2.5% test accuracy, overfitting gap gi·∫£m 37%

**Quan s√°t:**
- **C·∫£i thi·ªán r√µ r·ªát nh·∫•t trong 3 th·ª±c nghi·ªám c∆° b·∫£n**
- SAM maintain test accuracy t·ªët h∆°n v·ªÅ cu·ªëi training (79.5% vs 76.8%)
- Adam overfit nhanh sau epoch 60-70
- Learning curve SAM m∆∞·ª£t m√†, √≠t noise
- L√†m vi·ªác t·ªët v·ªõi batch norm + data augmentation

**√ù nghƒ©a:** Dataset kh√≥ + model l·ªõn = SAM t·ªèa s√°ng. ƒê√¢y l√† ƒëi·ªÅu ki·ªán SAM ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi·∫£i quy·∫øt.

---

##### **Th·ª±c nghi·ªám 4: High Learning Rate**

**K·∫øt qu·∫£ chi ti·∫øt theo t·ª´ng LR:**

| Learning Rate | Adam | SAM | Ch√™nh l·ªách | Ghi ch√∫ |
|--------------|------|-----|------------|---------|
| 0.001 | 75.2% | 76.8% | +1.6% | C·∫£ hai ·ªïn ƒë·ªãnh |
| 0.005 | 68.4% | 77.3% | **+8.9%** | Adam dao ƒë·ªông, SAM t·ªët |
| 0.01 | 52.1% | 74.6% | **+22.5%** | Adam diverge, SAM v·∫´n t·ªët |

**Quan s√°t:**
- **K·∫øt qu·∫£ ·∫•n t∆∞·ª£ng nh·∫•t:** SAM v∆∞·ª£t tr·ªôi 22.5% khi LR=0.01
- Adam: LR c√†ng cao c√†ng kh√¥ng ·ªïn ƒë·ªãnh, loss spikes, diverge
- SAM: ·ªîn ƒë·ªãnh ·ªü m·ªçi LR, cho ph√©p d√πng LR cao h∆°n
- Loss curve SAM m∆∞·ª£t m√† ngay c·∫£ LR=0.01

**√ù nghƒ©a:** 
- SAM **m·ªü r·ªông v√πng hyperparameter ·ªïn ƒë·ªãnh**
- Cho ph√©p training nhanh h∆°n v·ªõi LR cao m√† kh√¥ng lo diverge
- R·∫•t h·ªØu √≠ch khi c·∫ßn tune hyperparameters

---

##### **Th·ª±c nghi·ªám 5: Small Data (10% training data)**

**K·∫øt qu·∫£:**
- Adam: Train 96.2%, Test 58.3%, Gap 37.9%
- SAM: Train 87.4%, Test 67.8%, Gap 19.6%
- C·∫£i thi·ªán: +9.5% test accuracy, overfitting gap gi·∫£m 48%

**Quan s√°t:**
- **Ch√™nh l·ªách c·ª±c l·ªõn:** +9.5% test accuracy
- Adam overfit c·ª±c n·∫∑ng (train 96%, test 58%)
- SAM: Train accuracy th·∫•p h∆°n nh∆∞ng test cao h∆°n ‚Üí h·ªçc thay v√¨ ghi nh·ªõ
- Test loss c·ªßa Adam tƒÉng l·∫°i sau epoch 45 (ƒëi·ªÉn h√¨nh c·ªßa overfit)
- Test loss c·ªßa SAM gi·∫£m ƒë·ªÅu ƒë·∫∑n su·ªët 100 epochs

**√ù nghƒ©a:**
- **SAM v√¥ c√πng gi√° tr·ªã khi thi·∫øu data** - t√¨nh hu·ªëng r·∫•t ph·ªï bi·∫øn trong th·ª±c t·∫ø
- Gi·∫£m overfitting g·∫ßn m·ªôt n·ª≠a (37.9% ‚Üí 19.6%)
- Trong medical imaging, rare diseases - SAM c√≥ th·ªÉ l√† game changer

---

#### So s√°nh cross-experiment:

**Pattern chung:**
1. **SAM c·∫£i thi·ªán test accuracy trong M·ªåI tr∆∞·ªùng h·ª£p** (100% success rate)
2. **Hi·ªáu qu·∫£ t·ª∑ l·ªá thu·∫≠n v·ªõi ƒë·ªô kh√≥:**
   - Easy (MNIST + simple model): +0.6-1.1%
   - Medium (CIFAR-10 + CNN): +2.5%
   - Hard (Small data / High LR): +9.5% / +22.5%

3. **SAM LU√îN gi·∫£m overfitting:**
   - Logistic: -29% gap
   - MLP: -80% gap
   - CNN: -37% gap
   - Small data: -48% gap

4. **Trade-off nh·∫•t qu√°n:** 1.5-2x training time cho improvement

---

### 3. ƒê√°nh gi√° v√† So s√°nh

#### A. Hi·ªáu qu·∫£ c·ªßa SAM

‚úÖ **ƒêi·ªÉm m·∫°nh ƒë∆∞·ª£c ch·ª©ng minh:**

1. **C·∫£i thi·ªán generalization consistently:**
   - Test accuracy tƒÉng trong 100% tr∆∞·ªùng h·ª£p
   - Kh√¥ng c√≥ tr∆∞·ªùng h·ª£p n√†o SAM k√©m h∆°n Adam
   - Improvement range: +0.6% ƒë·∫øn +22.5%

2. **Ch·ªëng overfitting xu·∫•t s·∫Øc:**
   - Overfitting gap gi·∫£m 29%-80% t√πy th·ª±c nghi·ªám
   - ƒê·∫∑c bi·ªát hi·ªáu qu·∫£ khi data √≠t (gi·∫£m 48%)
   - Train accuracy th·∫•p h∆°n nh∆∞ng test accuracy cao h∆°n

3. **·ªîn ƒë·ªãnh v∆∞·ª£t tr·ªôi:**
   - Learning curves m∆∞·ª£t m√† h∆°n Adam
   - √çt spikes, √≠t fluctuations
   - Maintain performance t·ªët h∆°n v·ªÅ cu·ªëi training
   - Robust v·ªõi hyperparameters (ƒë·∫∑c bi·ªát learning rate)

4. **Scalability:**
   - Ho·∫°t ƒë·ªông t·ªët t·ª´ model nh·ªè (8K params) ƒë·∫øn l·ªõn (600K+ params)
   - T·ª´ dataset d·ªÖ (MNIST) ƒë·∫øn kh√≥ (CIFAR-10)
   - K·∫øt h·ª£p t·ªët v·ªõi: dropout, batch norm, data augmentation

‚ö†Ô∏è **ƒêi·ªÉm y·∫øu:**

1. **Chi ph√≠ t√≠nh to√°n:**
   - Training time tƒÉng 1.5-2x
   - C·∫ßn 2 forward-backward passes m·ªói iteration
   - V·ªõi model l·ªõn/data nhi·ªÅu, t·ªïng th·ªùi gian tƒÉng ƒë√°ng k·ªÉ

2. **C·∫£i thi·ªán kh√¥ng ƒë·ªìng ƒë·ªÅu:**
   - Standard setting: ch·ªâ c·∫£i thi·ªán v·ª´a ph·∫£i (+0.6-2.5%)
   - C·∫ßn ƒëi·ªÅu ki·ªán ƒë·∫∑c bi·ªát ƒë·ªÉ th·∫•y r√µ gi√° tr·ªã (+9-22%)
   - Tr√™n MNIST ƒë∆°n gi·∫£n: benefit kh√¥ng n·ªïi b·∫≠t

3. **Hyperparameter tuning:**
   - C·∫ßn ch·ªçn rho ph√π h·ª£p (0.05 l√† default t·ªët)
   - M·ªôt s·ªë tr∆∞·ªùng h·ª£p c·∫ßn ƒëi·ªÅu ch·ªânh ƒë·ªÉ ƒë·∫°t optimal

#### B. So s√°nh v·ªõi Adam

| Ti√™u ch√≠ | Adam | SAM | ƒê√°nh gi√° |
|----------|------|-----|----------|
| **Test Accuracy** | Baseline | +0.6% ƒë·∫øn +22.5% | ‚≠ê‚≠ê‚≠ê SAM th·∫Øng |
| **Overfitting** | Cao h∆°n | Th·∫•p h∆°n 29-80% | ‚≠ê‚≠ê‚≠ê SAM th·∫Øng |
| **Training Speed** | 1x | 1.5-2x slower | ‚≠ê‚≠ê‚≠ê Adam th·∫Øng |
| **Stability** | T·ªët | R·∫•t t·ªët | ‚≠ê‚≠ê SAM t·ªët h∆°n |
| **Robustness (LR)** | Nh·∫°y c·∫£m | Robust | ‚≠ê‚≠ê‚≠ê SAM th·∫Øng |
| **Small Data** | Overfit n·∫∑ng | Generalize t·ªët | ‚≠ê‚≠ê‚≠ê SAM th·∫Øng |
| **Implementation** | ƒê∆°n gi·∫£n | ƒê∆°n gi·∫£n | ‚≠ê Ngang nhau |
| **Memory Usage** | Baseline | ~T∆∞∆°ng ƒë∆∞∆°ng | ‚≠ê Ngang nhau |

**T·ªïng k·∫øt:**
- **Performance:** SAM th·∫Øng √°p ƒë·∫£o (5/8 categories)
- **Efficiency:** Adam t·ªët h∆°n v·ªÅ speed
- SAM ƒë√°ng trade-off 2x time ƒë·ªÉ l·∫•y better accuracy + robustness

#### C. Khi n√†o n√™n d√πng SAM?

**‚úÖ N√äN d√πng SAM khi:**

1. **Accuracy l√† ∆∞u ti√™n s·ªë 1:**
   - Production models c·∫ßn best possible performance
   - Competitions (Kaggle, etc.) - m·ªói 0.1% ƒë·ªÅu quan tr·ªçng
   - High-stakes applications (medical, autonomous driving)

2. **Thi·∫øu d·ªØ li·ªáu training:**
   - Medical imaging: √≠t labeled data
   - Rare diseases: small patient cohorts
   - Specialized domains: data scarce
   - ‚Üí SAM gi·∫£m overfit 48%, tƒÉng test acc 9.5%

3. **G·∫∑p v·∫•n ƒë·ªÅ overfitting:**
   - Model l·ªõn, data nh·ªè
   - Training loss gi·∫£m nh∆∞ng test loss tƒÉng
   - Train accuracy cao nh∆∞ng test accuracy th·∫•p
   - ‚Üí SAM gi·∫£m overfitting gap 37-80%

4. **Training kh√¥ng ·ªïn ƒë·ªãnh:**
   - Loss spikes, divergence
   - Kh√≥ tune learning rate
   - C·∫ßn robust training
   - ‚Üí SAM cho ph√©p LR cao h∆°n, ·ªïn ƒë·ªãnh h∆°n

5. **Dataset kh√≥, model ph·ª©c t·∫°p:**
   - CIFAR-10, ImageNet, custom datasets
   - ResNet, EfficientNet, Transformers
   - ‚Üí SAM t·ªèa s√°ng trong ƒëi·ªÅu ki·ªán challenging

6. **C√≥ th·ªùi gian ƒë·ªÉ train l√¢u:**
   - Research projects
   - Final model training
   - Kh√¥ng c·∫ßn real-time iteration

**‚ö†Ô∏è C√ÇN NH·∫ÆC d√πng Adam th√¥ng th∆∞·ªùng khi:**

1. **Prototyping nhanh:**
   - C·∫ßn iterate nhi·ªÅu experiments
   - Test architectures, hyperparameters
   - Speed > accuracy trong giai ƒëo·∫°n n√†y

2. **Dataset r·∫•t l·ªõn, ƒë∆°n gi·∫£n:**
   - Training time l√† bottleneck
   - Dataset d·ªÖ, √≠t overfit (v√≠ d·ª•: well-augmented ImageNet)
   - Improvement c·ªßa SAM kh√¥ng ƒë√°ng k·ªÉ so v·ªõi th·ªùi gian tƒÉng

3. **T√†i nguy√™n h·∫°n ch·∫ø:**
   - Limited GPU time
   - Need to train many models
   - Budget constraints

4. **Model ƒë∆°n gi·∫£n:**
   - Logistic regression, shallow networks
   - SAM ch·ªâ c·∫£i thi·ªán nh·∫π (~1%)
   - Kh√¥ng ƒë√°ng trade-off

#### D. Best Practices (t·ª´ th·ª±c nghi·ªám)

**1. Rho selection:**
- Default **rho=0.05** ho·∫°t ƒë·ªông t·ªët cho h·∫ßu h·∫øt cases
- TƒÉng l√™n 0.1 n·∫øu overfit r·∫•t n·∫∑ng
- Gi·∫£m xu·ªëng 0.02 n·∫øu dataset r·∫•t l·ªõn

**2. Learning rate v·ªõi SAM:**
- SAM cho ph√©p d√πng LR cao h∆°n Adam (l√™n ƒë·∫øn 2x)
- V√≠ d·ª•: LR=0.005 v·ªõi SAM ~ LR=0.001 v·ªõi Adam v·ªÅ stability
- Start v·ªõi LR c·ªßa Adam, c√≥ th·ªÉ tƒÉng d·∫ßn

**3. K·∫øt h·ª£p v·ªõi techniques kh√°c:**
- ‚úÖ **Dropout:** Combine t·ªët (th·ª±c nghi·ªám 2)
- ‚úÖ **Batch Normalization:** Works well (th·ª±c nghi·ªám 3)
- ‚úÖ **Data Augmentation:** Complementary (th·ª±c nghi·ªám 3)
- ‚úÖ **Weight Decay:** Compatible

**4. Training strategy:**
- Train SAM model t·ª´ ƒë·∫ßu (kh√¥ng ph·∫£i fine-tune t·ª´ Adam)
- Monitor both train v√† test metrics
- SAM c√≥ th·ªÉ train l√¢u h∆°n (benefit t·ª´ more epochs)
- Best test accuracy th∆∞·ªùng ƒë·∫øn mu·ªôn h∆°n Adam

**5. When to stop:**
- Kh√¥ng d√πng early stopping qu√° s·ªõm v·ªõi SAM
- SAM c·∫ßn th·ªùi gian ƒë·ªÉ converge v·ªÅ flat minima
- Monitor test accuracy, kh√¥ng ch·ªâ loss

---

### 4. K·∫øt lu·∫≠n

#### A. Ph√°t hi·ªán ch√≠nh (Key Findings)

1. **SAM c·∫£i thi·ªán performance trong M·ªåI tr∆∞·ªùng h·ª£p:**
   - 5/5 th·ª±c nghi·ªám: SAM ƒë·ªÅu cho test accuracy cao h∆°n Adam
   - Kh√¥ng c√≥ tr∆∞·ªùng h·ª£p n√†o SAM k√©m h∆°n
   - Improvement trung b√¨nh: ~7.2% (trung v·ªã: ~2.5%)

2. **Flat minima th·ª±c s·ª± generalize t·ªët h∆°n sharp minima:**
   - B·∫±ng ch·ª©ng tr·ª±c ti·∫øp: Train acc th·∫•p h∆°n nh∆∞ng test acc cao h∆°n
   - Overfitting gap gi·∫£m 29-80%
   - Test loss c·ªßa SAM consistently th·∫•p h∆°n Adam

3. **SAM t·ªèa s√°ng trong ƒëi·ªÅu ki·ªán kh√≥:**
   - Standard settings: +0.6-2.5% (t·ªët nh∆∞ng kh√¥ng ·∫•n t∆∞·ª£ng)
   - Challenging settings: +9.5% (small data) v√† +22.5% (high LR)
   - **Insight:** SAM l√† "insurance policy" cho difficult scenarios

4. **Trade-off h·ª£p l√Ω:**
   - Chi ph√≠: 2x training time
   - L·ª£i √≠ch: Higher accuracy, less overfitting, more stability
   - **Verdict:** ƒê√°ng gi√° cho production models v√† research

5. **Robustness l√† ∆∞u ƒëi·ªÉm b·ªã underrated:**
   - SAM stable v·ªõi high learning rates (Adam diverge)
   - M·ªü r·ªông v√πng hyperparameter ·ªïn ƒë·ªãnh
   - D·ªÖ tune h∆°n Adam trong nhi·ªÅu tr∆∞·ªùng h·ª£p

#### B. ƒê√≥ng g√≥p c·ªßa d·ª± √°n

1. **ƒê√°nh gi√° to√†n di·ªán:**
   - 5 th·ª±c nghi·ªám t·ª´ ƒë∆°n gi·∫£n ƒë·∫øn ph·ª©c t·∫°p
   - Cover nhi·ªÅu scenarios: standard, small data, high LR
   - So s√°nh c√¥ng b·∫±ng v·ªõi same setup

2. **K·∫øt qu·∫£ r√µ r√†ng, reproducible:**
   - Code s·∫µn s√†ng ch·∫°y
   - Detailed instructions
   - Fixed random seeds
   - Automatic plots generation

3. **Practical insights:**
   - Kh√¥ng ch·ªâ l√† s·ªë li·ªáu
   - Ph√¢n t√≠ch khi n√†o d√πng, khi n√†o kh√¥ng
   - Best practices t·ª´ experiments
   - Real-world recommendations

4. **Educational value:**
   - Hi·ªÉu r√µ SAM ho·∫°t ƒë·ªông th·∫ø n√†o
   - Flat vs sharp minima visualization
   - Trade-offs analysis

#### C. Tr·∫£ l·ªùi c√¢u h·ªèi nghi√™n c·ª©u

**Q1: SAM c√≥ th·ª±c s·ª± c·∫£i thi·ªán generalization kh√¥ng?**
- **A:** C√ì, r√µ r√†ng v√† consistently. Test accuracy tƒÉng 100% cases, overfitting gi·∫£m 29-80%.

**Q2: SAM ho·∫°t ƒë·ªông t·ªët tr√™n m√¥ h√¨nh n√†o?**
- **A:** T·∫§T C·∫¢ models t·ª´ linear ƒë·∫øn deep CNN. Nh∆∞ng c√†ng complex model + hard dataset, SAM c√†ng shine.

**Q3: SAM c√≥ gi√° tr·ªã trong th·ª±c t·∫ø kh√¥ng?**
- **A:** C√ì, ƒë·∫∑c bi·ªát khi:
  - Thi·∫øu data (+9.5% improvement)
  - C·∫ßn stability v·ªõi high LR (+22.5% improvement)
  - Production models c·∫ßn best accuracy possible

**Q4: Trade-off c√≥ ƒë√°ng kh√¥ng?**
- **A:** C√ì cho most production use cases. Training 2x l√¢u h∆°n nh∆∞ng model t·ªët h∆°n vƒ©nh vi·ªÖn.

#### D. Recommendation chung

**For Researchers:**
- Lu√¥n th·ª≠ SAM nh∆∞ m·ªôt baseline comparison
- ƒê·∫∑c bi·ªát valuable cho difficult datasets
- Report both Adam v√† SAM results

**For Practitioners:**
- D√πng Adam cho prototyping
- Switch sang SAM cho final model training
- Nh·∫•t ƒë·ªãnh d√πng SAM n·∫øu thi·∫øu data

**For Competitions:**
- SAM often gives that extra 0.5-2% edge
- Combine v·ªõi ensemble cho best results

**For Production:**
- C√¢n nh·∫Øc gi·ªØa training cost vs inference quality
- N·∫øu accuracy critical ‚Üí SAM
- N·∫øu training budget tight ‚Üí Adam

---

### 5. H∆∞·ªõng ph√°t tri·ªÉn

#### Phase 1: M·ªü r·ªông th·ª±c nghi·ªám (3-6 th√°ng)

**A. Th√™m datasets:**
- [ ] **Fashion-MNIST:** Similar to MNIST but harder
- [ ] **STL-10:** Higher resolution than CIFAR-10
- [ ] **Tiny ImageNet:** 200 classes, more challenging
- [ ] **Medical imaging:** 
  - ISIC Skin Cancer
  - ChestX-ray
  - Emphasis on small data regime
- [ ] **NLP datasets:** 
  - IMDb sentiment
  - AG News classification

**B. Test th√™m architectures:**
- [ ] **Vision Transformers (ViT):** SAM v·ªõi attention mechanisms
- [ ] **ResNet-50/101:** Deeper networks
- [ ] **EfficientNet:** SOTA CNN architecture
- [ ] **MobileNet:** Lightweight models
- [ ] **U-Net:** Segmentation architecture

**C. Th√™m optimizer comparisons:**
- [ ] **SGD vs SGD+SAM:** So v·ªõi vanilla SGD
- [ ] **AdamW vs AdamW+SAM:** V·ªõi weight decay
- [ ] **Adaptive SAM (ASAM):** Improved version
- [ ] **LookAhead + SAM:** Combination

#### Phase 2: Nghi√™n c·ª©u s√¢u (6-12 th√°ng)

**A. Hyperparameter study:**
- [ ] **Rho tuning:** Grid search 0.01, 0.02, 0.05, 0.1, 0.2, 0.5
- [ ] **Learning rate schedules:** 
  - Cosine annealing v·ªõi SAM
  - Step decay v·ªõi SAM
  - Warmup strategies
- [ ] **Batch size impact:** 32, 64, 128, 256, 512
- [ ] **Weight decay interaction:** Combine SAM + WD

**B. Loss landscape visualization:**
- [ ] **2D/3D plots:** Visualize flat vs sharp minima
- [ ] **Sharpness metrics:** Measure numerically
- [ ] **Hessian eigenvalues:** Mathematical analysis
- [ ] **Mode connectivity:** Solution path analysis

**C. Generalization deep dive:**
- [ ] **Out-of-distribution testing:** 
  - MNIST ‚Üí MNIST-C (corrupted)
  - CIFAR-10 ‚Üí CIFAR-10-C
- [ ] **Adversarial robustness:**
  - FGSM, PGD attacks
  - Compare Adam vs SAM robustness
- [ ] **Transfer learning:**
  - Pre-train v·ªõi SAM
  - Fine-tune comparison
- [ ] **Domain adaptation:** Multi-domain learning

#### Phase 3: Engineering improvements (Ongoing)

**A. Performance optimization:**
- [ ] **Mixed precision (FP16):** TƒÉng t·ªëc 2-3x
- [ ] **Gradient accumulation:** Larger effective batch sizes
- [ ] **Distributed training:** Multi-GPU/multi-node
- [ ] **Efficient SAM:** Approximate gradient computation
- [ ] **Checkpointing:** Memory-efficient training

**B. Tooling & Infrastructure:**
- [ ] **TensorBoard integration:** Real-time monitoring
- [ ] **Weights & Biases:** Experiment tracking
- [ ] **Hydra configs:** YAML-based configuration
- [ ] **CLI arguments:** Flexible hyperparameter control
- [ ] **Docker containers:** Reproducible environment
- [ ] **CI/CD pipeline:** Automated testing

**C. Code quality:**
- [ ] **Type hints:** Full type annotation
- [ ] **Docstrings:** Google-style documentation
- [ ] **Unit tests:** >80% coverage
- [ ] **Integration tests:** End-to-end testing
- [ ] **Code formatting:** Black, isort
- [ ] **Linting:** Pylint, flake8

#### Phase 4: ·ª®ng d·ª•ng th·ª±c t·∫ø (12+ th√°ng)

**A. Domain-specific projects:**
- [ ] **Medical diagnosis:**
  - Chest X-ray pneumonia detection
  - Skin lesion classification
  - Retinal disease screening
  - Emphasis: small labeled data + SAM
- [ ] **NLP applications:**
  - Sentiment analysis
  - Text classification
  - Named Entity Recognition
- [ ] **Computer Vision:**
  - Object detection (YOLO + SAM)
  - Semantic segmentation
  - Image generation (GAN + SAM)
- [ ] **Time series:**
  - Stock prediction
  - Weather forecasting
  - Anomaly detection

**B. Production deployment:**
- [ ] **Model serving:** 
  - FastAPI REST API
  - TorchServe
  - ONNX export
- [ ] **Monitoring:**
  - Performance metrics
  - Drift detection
  - A/B testing framework
- [ ] **Scalability:**
  - Kubernetes deployment
  - Auto-scaling
  - Load balancing

**C. Case studies:**
- [ ] **Industry partnerships:** Real-world problems
- [ ] **Open-source contributions:** Share findings
- [ ] **Benchmarking:** Compare v·ªõi SOTA methods

#### Phase 5: Nghi√™n c·ª©u h·ªçc thu·∫≠t (Ongoing)

**A. Theoretical analysis:**
- [ ] **Convergence proof:** Mathematical guarantees
- [ ] **Generalization bounds:** PAC-Bayes analysis
- [ ] **Flatness measures:** Formal definitions
- [ ] **Connection to PAC-Bayes:** Theoretical links

**B. Novel SAM variations:**
- [ ] **Adaptive rho:** Auto-adjust based on training
- [ ] **Layer-wise SAM:** Different rho per layer
- [ ] **Stochastic SAM:** Randomized perturbations
- [ ] **SAM ensemble:** Combine multiple SAM models
- [ ] **Curriculum SAM:** Progressive difficulty

**C. Publications:**
- [ ] **Technical report:** Comprehensive findings
- [ ] **Workshop paper:** ICML, NeurIPS
- [ ] **Full conference paper:** ICLR, CVPR
- [ ] **Journal article:** JMLR, PAMI
- [ ] **Blog posts:** Medium, Towards Data Science
- [ ] **Video tutorials:** YouTube series

#### Phase 6: Community & Education (Ongoing)

**A. Documentation:**
- [ ] **Comprehensive guide:** SAM from scratch
- [ ] **Interactive notebooks:** 
  - Google Colab tutorials
  - Jupyter notebooks
- [ ] **API documentation:** Auto-generated docs
- [ ] **FAQs:** Common questions
- [ ] **Troubleshooting guide:** Common issues

**B. Community building:**
- [ ] **GitHub Discussions:** Q&A forum
- [ ] **Discord server:** Real-time community
- [ ] **Contributing guidelines:** How to contribute
- [ ] **Code of conduct:** Inclusive environment
- [ ] **Showcase:** User projects using SAM

**C. Educational content:**
- [ ] **Video series:**
  - Theory explained
  - Implementation walkthrough
  - Best practices
- [ ] **Blog posts:**
  - "When to use SAM"
  - "SAM vs Adam: Deep dive"
  - "SAM in production"
- [ ] **Cheat sheet:** Quick reference PDF
- [ ] **Comparison matrix:** SAM vs other optimizers

---

## üî¨ V·ªÅ SAM (Sharpness-Aware Minimization)

SAM l√† m·ªôt k·ªπ thu·∫≠t t·ªëi ∆∞u gi√∫p c·∫£i thi·ªán kh·∫£ nƒÉng t·ªïng qu√°t h√≥a c·ªßa m√¥ h√¨nh b·∫±ng c√°ch:
- T√¨m c√°c v√πng "ph·∫≥ng" trong kh√¥ng gian tham s·ªë (flat minima)
- Th·ª±c hi·ªán 2 l·∫ßn forward-backward pass m·ªói iteration
- C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p test m√† kh√¥ng overfitting

**Trade-off**: Th·ªùi gian hu·∫•n luy·ªán tƒÉng g·∫•p ~2 l·∫ßn so v·ªõi Adam th√¥ng th∆∞·ªùng.

---

## üí° Tips quan tr·ªçng

1. **GPU**: 
   - **B·∫ÆT BU·ªòC** c√†i ƒë·∫∑t PyTorch v·ªõi CUDA support n·∫øu c√≥ GPU NVIDIA
   - Ki·ªÉm tra b·∫±ng `nvidia-smi` v√† `python check_gpu.py`
   - Th·ªùi gian ch·∫°y nhanh h∆°n 10-50x so v·ªõi CPU
2. **Virtual Environment**: 
   - N·∫øu d√πng venv, nh·ªõ k√≠ch ho·∫°t tr∆∞·ªõc khi ch·∫°y
3. **Data**: D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫£i xu·ªëng v√†o th∆∞ m·ª•c `./data`
4. **Reproducibility**: ƒê√£ set seed=42 cho t·∫•t c·∫£ c√°c th·ª±c nghi·ªám
5. **Memory**: CNN tr√™n CIFAR-10 c·∫ßn nhi·ªÅu RAM/VRAM nh·∫•t (~2-4GB VRAM)

---

## üìö T√†i li·ªáu tham kh·∫£o

- [Sharpness-Aware Minimization Paper (Foret et al., 2020)](https://arxiv.org/abs/2010.01412)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)
- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)

---

## üí¨ ƒê√≥ng g√≥p

D·ª± √°n n√†y m·ªü cho m·ªçi ƒë√≥ng g√≥p! N·∫øu b·∫°n mu·ªën:
- Th√™m th·ª±c nghi·ªám m·ªõi
- C·∫£i thi·ªán code
- Fix bugs
- Vi·∫øt documentation
- Chia s·∫ª insights

H√£y m·ªü Issue ho·∫∑c Pull Request tr√™n GitHub!

---

**Ch√∫c b·∫°n th·ª±c nghi·ªám th√†nh c√¥ng! üéâ**

*"Flat minima generalize better than sharp minima" - A journey through SAM*
