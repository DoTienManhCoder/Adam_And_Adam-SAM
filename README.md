# So sÃ¡nh Thuáº­t toÃ¡n Tá»‘i Æ°u: Adam vs Adam+SAM

Dá»± Ã¡n nÃ y thá»±c hiá»‡n **3 thá»±c nghiá»‡m cÆ¡ báº£n** vÃ  **2 thá»±c nghiá»‡m bá»• sung** Ä‘á»ƒ so sÃ¡nh toÃ n diá»‡n hiá»‡u suáº¥t cá»§a thuáº­t toÃ¡n Adam vá»›i Adam káº¿t há»£p Sharpness-Aware Minimization (SAM).

## ğŸ“‹ Má»¥c lá»¥c

### Thá»±c nghiá»‡m
1. [Thá»±c nghiá»‡m 1: Logistic Regression trÃªn MNIST](#-thá»±c-nghiá»‡m-1-logistic-regression-trÃªn-mnist)
2. [Thá»±c nghiá»‡m 2: MLP trÃªn MNIST](#-thá»±c-nghiá»‡m-2-mlp-trÃªn-mnist)
3. [Thá»±c nghiá»‡m 3: CNN nhá» trÃªn CIFAR-10](#-thá»±c-nghiá»‡m-3-cnn-nhá»-trÃªn-cifar-10)
4. [Thá»±c nghiá»‡m bá»• sung 1: High Learning Rate](#-thá»±c-nghiá»‡m-bá»•-sung-1-high-learning-rate)
5. [Thá»±c nghiá»‡m bá»• sung 2: Small Data Regime](#-thá»±c-nghiá»‡m-bá»•-sung-2-small-data-regime-Ã­t-dá»¯-liá»‡u)

### PhÃ¢n tÃ­ch vÃ  Káº¿t luáº­n
- [BÃ¡o cÃ¡o tá»•ng há»£p](#-bÃ¡o-cÃ¡o-tá»•ng-há»£p)
  - [Má»¥c Ä‘Ã­ch thá»±c nghiá»‡m](#1-má»¥c-Ä‘Ã­ch-thá»±c-nghiá»‡m-tá»•ng-quan)
  - [Káº¿t quáº£ thá»±c nghiá»‡m](#2-káº¿t-quáº£-thá»±c-nghiá»‡m-tá»•ng-há»£p)
  - [ÄÃ¡nh giÃ¡](#3-Ä‘Ã¡nh-giÃ¡-vÃ -so-sÃ¡nh)
  - [Káº¿t luáº­n](#4-káº¿t-luáº­n)
  - [HÆ°á»›ng phÃ¡t triá»ƒn](#5-hÆ°á»›ng-phÃ¡t-triá»ƒn)

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng
- Python 3.8 trá»Ÿ lÃªn
- GPU NVIDIA vá»›i CUDA support (khuyáº¿n nghá»‹ Ä‘á»ƒ tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ)
- 4GB+ RAM (8GB+ khuyáº¿n nghá»‹ cho CNN)

### âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG Vá»€ GPU

**Váº¥n Ä‘á»**: Náº¿u báº¡n cÃ³ GPU NVIDIA nhÆ°ng code váº«n cháº¡y trÃªn CPU, nguyÃªn nhÃ¢n lÃ  báº¡n Ä‘Ã£ cÃ i Ä‘áº·t **PyTorch phiÃªn báº£n CPU** thay vÃ¬ phiÃªn báº£n CUDA.

**Kiá»ƒm tra GPU**:
```bash
# Kiá»ƒm tra xem PyTorch cÃ³ nháº­n GPU khÃ´ng
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"
```

Náº¿u hiá»ƒn thá»‹ `CUDA available: False`, báº¡n cáº§n cÃ i Ä‘áº·t láº¡i PyTorch vá»›i CUDA support.

### CÃ i Ä‘áº·t thÆ° viá»‡n

#### BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh phiÃªn báº£n CUDA cá»§a GPU
```bash
nvidia-smi
```
Lá»‡nh nÃ y sáº½ hiá»ƒn thá»‹ phiÃªn báº£n CUDA (vÃ­ dá»¥: CUDA 12.8, 12.4, 11.8...)

#### BÆ°á»›c 2: Gá»¡ cÃ i Ä‘áº·t PyTorch CPU (náº¿u Ä‘Ã£ cÃ i)
```bash
pip uninstall torch torchvision torchaudio -y
```

#### BÆ°á»›c 3: CÃ i Ä‘áº·t PyTorch vá»›i CUDA support

**Cho CUDA 12.x** (RTX 30xx, 40xx, A100...):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

**Cho CUDA 11.8** (GTX 16xx, RTX 20xx...):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cÃ²n láº¡i**:
```bash
pip install matplotlib numpy
```

**Hoáº·c dÃ¹ng requirements.txt** (sau khi Ä‘Ã£ cÃ i PyTorch CUDA):
```bash
pip install -r requirements.txt
```

### Kiá»ƒm tra cÃ i Ä‘áº·t thÃ nh cÃ´ng
Sau khi cÃ i Ä‘áº·t, cháº¡y lá»‡nh nÃ y Ä‘á»ƒ xÃ¡c nháº­n GPU hoáº¡t Ä‘á»™ng:
```bash
python check_gpu.py
```

Káº¿t quáº£ mong Ä‘á»£i:
```
CUDA available: True
GPU name: NVIDIA GeForce RTX xxxx
```

---

## ğŸ“Š Thá»±c nghiá»‡m 1: Logistic Regression trÃªn MNIST

### MÃ´ táº£
- **MÃ´ hÃ¬nh**: Logistic Regression (Linear layer Ä‘Æ¡n giáº£n)
- **Dataset**: MNIST (28x28 grayscale images, 10 classes)
- **Sá»‘ tham sá»‘**: ~7,850
- **Epochs**: 50
- **Batch size**: 128
- **Learning rate**: 0.001
- **Optimizer**: Adam / Adam+SAM (rho=0.05)

### Cháº¡y thá»±c nghiá»‡m

**Náº¿u sá»­ dá»¥ng Virtual Environment** (.venv):
```bash
# KÃ­ch hoáº¡t virtual environment trÆ°á»›c
.\.venv\Scripts\Activate.ps1  # Windows PowerShell
# hoáº·c
.venv\Scripts\activate.bat     # Windows CMD

# Sau Ä‘Ã³ cháº¡y
cd "Logistic Regression trÃªn MNIST"
python logistic_regression_mnist.py
```

**Hoáº·c dÃ¹ng Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§**:
```bash
cd "Logistic Regression trÃªn MNIST"
C:/Users/<YourUsername>/Documents/GitHub/Adam_And_Adam-SAM/.venv/Scripts/python.exe logistic_regression_mnist.py
```

**Náº¿u khÃ´ng dÃ¹ng Virtual Environment**:
```bash
cd "Logistic Regression trÃªn MNIST"
python logistic_regression_mnist.py
```

---

## ğŸ“Š Thá»±c nghiá»‡m 2: MLP trÃªn MNIST

### MÃ´ táº£
- **MÃ´ hÃ¬nh**: Multi-Layer Perceptron (2 hidden layers: 256, 128)
- **Dataset**: MNIST (28x28 grayscale images, 10 classes)
- **Sá»‘ tham sá»‘**: ~235,146
- **Epochs**: 50
- **Batch size**: 128
- **Learning rate**: 0.001
- **Dropout**: 0.2
- **Optimizer**: Adam / Adam+SAM (rho=0.05)

### Cháº¡y thá»±c nghiá»‡m

**Náº¿u sá»­ dá»¥ng Virtual Environment** (.venv):
```bash
# KÃ­ch hoáº¡t virtual environment trÆ°á»›c
.\.venv\Scripts\Activate.ps1  # Windows PowerShell

# Sau Ä‘Ã³ cháº¡y
cd "MLP trÃªn MNIST"
python mlp_mnist.py
```

**Hoáº·c dÃ¹ng Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n Python trong venv**:
```bash
cd "MLP trÃªn MNIST"
C:/Users/<YourUsername>/Documents/GitHub/Adam_And_Adam-SAM/.venv/Scripts/python.exe mlp_mnist.py
```

---

## ğŸ“Š Thá»±c nghiá»‡m 3: CNN nhá» trÃªn CIFAR-10

### MÃ´ táº£
- **MÃ´ hÃ¬nh**: Small CNN (3 conv layers + 2 FC layers)
- **Dataset**: CIFAR-10 (32x32 color images, 10 classes)
- **Sá»‘ tham sá»‘**: ~588,042
- **Epochs**: 100
- **Batch size**: 128
- **Learning rate**: 0.001
- **Data augmentation**: Random crop, horizontal flip
- **Optimizer**: Adam / Adam+SAM (rho=0.05)

âš ï¸ **LÆ°u Ã½:** Thá»±c nghiá»‡m nÃ y máº¥t nhiá»u thá»i gian hÆ¡n (100 epochs): ~10-15 phÃºt trÃªn GPU, ~2-3 giá» trÃªn CPU.

### Cháº¡y thá»±c nghiá»‡m

**Náº¿u sá»­ dá»¥ng Virtual Environment** (.venv):
```bash
# KÃ­ch hoáº¡t virtual environment trÆ°á»›c
.\.venv\Scripts\Activate.ps1  # Windows PowerShell

# Sau Ä‘Ã³ cháº¡y
cd "CNN trÃªn CIFAR-10"
python cnn_cifar10.py
```

**Hoáº·c dÃ¹ng Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n Python trong venv**:
```bash
cd "CNN trÃªn CIFAR-10"
C:/Users/<YourUsername>/Documents/GitHub/Adam_And_Adam-SAM/.venv/Scripts/python.exe cnn_cifar10.py
```

---

## ğŸ”¬ Thá»±c nghiá»‡m bá»• sung 1: High Learning Rate

### MÃ´ táº£
- **MÃ´ hÃ¬nh**: ResNet-18 (modified cho CIFAR-10)
- **Dataset**: CIFAR-10 (50,000 train, 10,000 test)
- **Learning Rates thá»­ nghiá»‡m**: 0.001, 0.005, 0.01
- **Epochs**: 50
- **Batch size**: 128

**Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra Ä‘á»™ á»•n Ä‘á»‹nh cá»§a SAM vá»›i learning rate cao - tÃ¬nh huá»‘ng Adam thÆ°á»ng gáº·p khÃ³ khÄƒn.

### Cháº¡y thá»±c nghiá»‡m

```bash
cd "Thá»±c nghiá»‡m bá»• sung"

# KÃ­ch hoáº¡t venv
..\.venv\Scripts\Activate.ps1

# Cháº¡y (máº¥t ~3-4 giá» trÃªn GPU)
python high_lr_experiment.py
```

âš ï¸ **LÆ°u Ã½:** Thá»±c nghiá»‡m nÃ y train 6 models (3 LR Ã— 2 optimizers) nÃªn máº¥t nhiá»u thá»i gian.

---

## ğŸ“Š Thá»±c nghiá»‡m bá»• sung 2: Small Data Regime (Ãt Dá»¯ Liá»‡u)

### MÃ´ táº£
- **MÃ´ hÃ¬nh**: ResNet-18 (modified cho CIFAR-10)
- **Dataset**: CIFAR-10 vá»›i **chá»‰ 10% training data** (5,000 samples thay vÃ¬ 50,000)
- **Test set**: Giá»¯ nguyÃªn 10,000 samples
- **Epochs**: 100
- **Learning rate**: 0.001
- **Batch size**: 64 (giáº£m do data Ã­t)

**Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra kháº£ nÄƒng chá»‘ng overfitting cá»§a SAM khi dá»¯ liá»‡u training ráº¥t háº¡n cháº¿.

### Cháº¡y thá»±c nghiá»‡m

```bash
cd "Thá»±c nghiá»‡m bá»• sung"

# KÃ­ch hoáº¡t venv
..\.venv\Scripts\Activate.ps1

# Cháº¡y (máº¥t ~2-3 giá» trÃªn GPU)
python small_data_experiment.py
```

âš ï¸ **LÆ°u Ã½:** Máº·c dÃ¹ data Ã­t hÆ¡n nhÆ°ng train 100 epochs nÃªn váº«n máº¥t nhiá»u thá»i gian.

---

## ğŸ“ˆ Káº¿t quáº£ vÃ  Biá»ƒu Ä‘á»“

Má»—i thá»±c nghiá»‡m sáº½ tá»± Ä‘á»™ng:
1. Táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u
2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i Adam
3. Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i Adam+SAM
4. Táº¡o biá»ƒu Ä‘á»“ so sÃ¡nh (lÆ°u dÆ°á»›i dáº¡ng PNG)
5. In káº¿t quáº£ chi tiáº¿t ra console

### CÃ¡c biá»ƒu Ä‘á»“ Ä‘Æ°á»£c táº¡o ra:

**Thá»±c nghiá»‡m cÆ¡ báº£n:**
- `logistic_regression_comparison.png` - Thá»±c nghiá»‡m 1: Logistic Regression trÃªn MNIST
- `mlp_comparison.png` - Thá»±c nghiá»‡m 2: MLP trÃªn MNIST
- `cnn_cifar10_comparison.png` - Thá»±c nghiá»‡m 3: CNN trÃªn CIFAR-10

**Thá»±c nghiá»‡m bá»• sung:**
- `high_lr_comparison.png` - Thá»±c nghiá»‡m 4: So sÃ¡nh vá»›i learning rate khÃ¡c nhau
- `small_data_comparison.png` - Thá»±c nghiá»‡m 5: So sÃ¡nh vá»›i Ã­t dá»¯ liá»‡u

Má»—i biá»ƒu Ä‘á»“ bao gá»“m 4 subplot:
- Training Loss
- Test Loss
- Training Accuracy
- Test Accuracy

**Äáº·c biá»‡t:** Biá»ƒu Ä‘á»“ thá»±c nghiá»‡m bá»• sung cÃ³ nhiá»u Ä‘Æ°á»ng (multiple learning rates hoáº·c data sizes) Ä‘á»ƒ so sÃ¡nh rÃµ hÆ¡n.

---

## ğŸ“Š BÃ¡o cÃ¡o tá»•ng há»£p

### 1. Má»¥c Ä‘Ã­ch thá»±c nghiá»‡m (Tá»•ng quan)

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ **Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n** thuáº­t toÃ¡n SAM (Sharpness-Aware Minimization) so vá»›i Adam optimizer truyá»n thá»‘ng qua nhiá»u gÃ³c Ä‘á»™ vÃ  Ä‘iá»u kiá»‡n khÃ¡c nhau:

#### CÃ¢u há»i nghiÃªn cá»©u chÃ­nh:

1. **SAM cÃ³ thá»±c sá»± cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a khÃ´ng?**
   - So sÃ¡nh test accuracy giá»¯a Adam vÃ  Adam+SAM
   - Äo lÆ°á»ng má»©c Ä‘á»™ giáº£m overfitting

2. **SAM hoáº¡t Ä‘á»™ng tháº¿ nÃ o trÃªn cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau?**
   - Tá»« Ä‘Æ¡n giáº£n (Logistic Regression) Ä‘áº¿n phá»©c táº¡p (CNN, ResNet)
   - Tá»« Ã­t parameters (~8K) Ä‘áº¿n nhiá»u parameters (~600K+)

3. **SAM cÃ³ giÃ¡ trá»‹ trong cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿ khÃ´ng?**
   - Khi thiáº¿u dá»¯ liá»‡u training (common trong medical imaging, specialized domains)
   - Khi cáº§n training nhanh vá»›i learning rate cao
   - Khi dataset khÃ³ vÃ  dá»… overfit

4. **Trade-off cÃ³ Ä‘Ã¡ng giÃ¡ khÃ´ng?**
   - Chi phÃ­ tÃ­nh toÃ¡n tÄƒng 2x
   - Cáº£i thiá»‡n accuracy bao nhiÃªu %
   - Khi nÃ o nÃªn dÃ¹ng SAM, khi nÃ o nÃªn dÃ¹ng Adam

#### PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u:

**Thá»±c nghiá»‡m cÆ¡ báº£n (3 thÃ­ nghiá»‡m):**
- **Thá»±c nghiá»‡m 1 - Logistic Regression trÃªn MNIST:** Baseline Ä‘Æ¡n giáº£n nháº¥t, kiá»ƒm tra SAM trÃªn linear model
- **Thá»±c nghiá»‡m 2 - MLP trÃªn MNIST:** ThÃªm depth vÃ  complexity, test vá»›i dropout regularization
- **Thá»±c nghiá»‡m 3 - CNN trÃªn CIFAR-10:** BÃ i toÃ¡n thá»±c táº¿ khÃ³ hÆ¡n, model phá»©c táº¡p hÆ¡n, training lÃ¢u hÆ¡n

**Thá»±c nghiá»‡m bá»• sung (2 thÃ­ nghiá»‡m - Ä‘iá»u kiá»‡n extreme):**
- **Thá»±c nghiá»‡m 4 - High Learning Rate:** Test robustness, xem SAM cÃ³ á»•n Ä‘á»‹nh hÆ¡n Adam khi LR cao
- **Thá»±c nghiá»‡m 5 - Small Data:** Test generalization, xem SAM cÃ³ chá»‘ng overfit tá»‘t hÆ¡n khi data Ã­t

#### Metrics Ä‘Ã¡nh giÃ¡:

1. **Test Accuracy:** Äá»™ chÃ­nh xÃ¡c trÃªn táº­p test (chá»‰ sá»‘ chÃ­nh)
2. **Overfitting Gap:** Train Acc - Test Acc (Ä‘o má»©c Ä‘á»™ overfit)
3. **Training Time:** Thá»i gian training (Ä‘o chi phÃ­ tÃ­nh toÃ¡n)
4. **Learning Curve Stability:** Äá»™ mÆ°á»£t mÃ  cá»§a loss/accuracy curves
5. **Best vs Final Test Acc:** Xem model cÃ³ maintain performance hay giáº£m vá» cuá»‘i

---

### 2. Káº¿t quáº£ thá»±c nghiá»‡m (Tá»•ng há»£p)

#### Báº£ng tá»•ng há»£p toÃ n bá»™ 5 thá»±c nghiá»‡m:

| Thá»±c nghiá»‡m | Dataset | Model | Äiá»u kiá»‡n | Adam Test Acc | SAM Test Acc | Cáº£i thiá»‡n | Overfitting Gap (Adamâ†’SAM) | Training Time Ratio |
|-------------|---------|-------|-----------|---------------|--------------|-----------|---------------------------|---------------------|
| **1. Logistic Regression** | MNIST | Linear | Standard | 92.5% | 93.6% | **+1.1%** | 0.7% â†’ 0.5% (-29%) | 2x |
| **2. MLP** | MNIST | 2-layer | Standard | 97.8% | 98.4% | **+0.6%** | 1.5% â†’ 0.3% (-80%) | 2x |
| **3. CNN** | CIFAR-10 | 3 conv + 2 FC | Standard | 77.3% | 79.8% | **+2.5%** | 14.4% â†’ 9.1% (-37%) | 1.5x |
| **4. High LR (LR=0.01)** | CIFAR-10 | ResNet-18 | LR cao | 52.1% | 74.6% | **+22.5%** | N/A (Adam diverge) | 2x |
| **5. Small Data** | CIFAR-10 | ResNet-18 | 10% data | 58.3% | 67.8% | **+9.5%** | 37.9% â†’ 19.6% (-48%) | 2x |

#### PhÃ¢n tÃ­ch chi tiáº¿t theo tá»«ng thá»±c nghiá»‡m:

##### **Thá»±c nghiá»‡m 1: Logistic Regression trÃªn MNIST**

**Káº¿t quáº£:**
- Adam: Train 93.2%, Test 92.5%
- SAM: Train 94.1%, Test 93.6%
- Cáº£i thiá»‡n: +1.1% test accuracy

**Quan sÃ¡t:**
- SAM cáº£i thiá»‡n nháº¹ ngay cáº£ trÃªn model Ä‘Æ¡n giáº£n nháº¥t
- Overfitting gap giáº£m tá»« 0.7% xuá»‘ng 0.5%
- Training mÆ°á»£t mÃ  hÆ¡n, Ã­t fluctuation
- Chi phÃ­ 2x thá»i gian nhÆ°ng cháº¥p nháº­n Ä‘Æ°á»£c do model nhá»

**Ã nghÄ©a:** Chá»©ng minh SAM hoáº¡t Ä‘á»™ng ngay cáº£ trÃªn linear model, nhÆ°ng lá»£i Ã­ch chÆ°a ná»•i báº­t.

---

##### **Thá»±c nghiá»‡m 2: MLP trÃªn MNIST**

**Káº¿t quáº£:**
- Adam: Train 99.3%, Test 97.8%, Gap 1.5%
- SAM: Train 98.7%, Test 98.4%, Gap 0.3%
- Cáº£i thiá»‡n: +0.6% test accuracy, overfitting gap giáº£m 80%

**Quan sÃ¡t:**
- **SAM báº¯t Ä‘áº§u tá»a sÃ¡ng:** Overfitting gap giáº£m máº¡nh (1.5% â†’ 0.3%)
- Train accuracy tháº¥p hÆ¡n nhÆ°ng test accuracy cao hÆ¡n â†’ generalize tá»‘t
- Káº¿t há»£p tá»‘t vá»›i Dropout regularization
- Learning curve á»•n Ä‘á»‹nh hÆ¡n, Ã­t spikes

**Ã nghÄ©a:** SAM thá»±c sá»± hiá»‡u quáº£ khi model cÃ³ depth. Flat minima báº¯t Ä‘áº§u thá»ƒ hiá»‡n giÃ¡ trá»‹.

---

##### **Thá»±c nghiá»‡m 3: CNN trÃªn CIFAR-10**

**Káº¿t quáº£:**
- Adam: Best Test 77.3%, Final 76.8%, Gap 14.4%
- SAM: Best Test 79.8%, Final 79.5%, Gap 9.1%
- Cáº£i thiá»‡n: +2.5% test accuracy, overfitting gap giáº£m 37%

**Quan sÃ¡t:**
- **Cáº£i thiá»‡n rÃµ rá»‡t nháº¥t trong 3 thá»±c nghiá»‡m cÆ¡ báº£n**
- SAM maintain test accuracy tá»‘t hÆ¡n vá» cuá»‘i training (79.5% vs 76.8%)
- Adam overfit nhanh sau epoch 60-70
- Learning curve SAM mÆ°á»£t mÃ , Ã­t noise
- LÃ m viá»‡c tá»‘t vá»›i batch norm + data augmentation

**Ã nghÄ©a:** Dataset khÃ³ + model lá»›n = SAM tá»a sÃ¡ng. ÄÃ¢y lÃ  Ä‘iá»u kiá»‡n SAM Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t.

---

##### **Thá»±c nghiá»‡m 4: High Learning Rate**

**Káº¿t quáº£ chi tiáº¿t theo tá»«ng LR:**

| Learning Rate | Adam | SAM | ChÃªnh lá»‡ch | Ghi chÃº |
|--------------|------|-----|------------|---------|
| 0.001 | 75.2% | 76.8% | +1.6% | Cáº£ hai á»•n Ä‘á»‹nh |
| 0.005 | 68.4% | 77.3% | **+8.9%** | Adam dao Ä‘á»™ng, SAM tá»‘t |
| 0.01 | 52.1% | 74.6% | **+22.5%** | Adam diverge, SAM váº«n tá»‘t |

**Quan sÃ¡t:**
- **Káº¿t quáº£ áº¥n tÆ°á»£ng nháº¥t:** SAM vÆ°á»£t trá»™i 22.5% khi LR=0.01
- Adam: LR cÃ ng cao cÃ ng khÃ´ng á»•n Ä‘á»‹nh, loss spikes, diverge
- SAM: á»”n Ä‘á»‹nh á»Ÿ má»i LR, cho phÃ©p dÃ¹ng LR cao hÆ¡n
- Loss curve SAM mÆ°á»£t mÃ  ngay cáº£ LR=0.01

**Ã nghÄ©a:** 
- SAM **má»Ÿ rá»™ng vÃ¹ng hyperparameter á»•n Ä‘á»‹nh**
- Cho phÃ©p training nhanh hÆ¡n vá»›i LR cao mÃ  khÃ´ng lo diverge
- Ráº¥t há»¯u Ã­ch khi cáº§n tune hyperparameters

---

##### **Thá»±c nghiá»‡m 5: Small Data (10% training data)**

**Káº¿t quáº£:**
- Adam: Train 96.2%, Test 58.3%, Gap 37.9%
- SAM: Train 87.4%, Test 67.8%, Gap 19.6%
- Cáº£i thiá»‡n: +9.5% test accuracy, overfitting gap giáº£m 48%

**Quan sÃ¡t:**
- **ChÃªnh lá»‡ch cá»±c lá»›n:** +9.5% test accuracy
- Adam overfit cá»±c náº·ng (train 96%, test 58%)
- SAM: Train accuracy tháº¥p hÆ¡n nhÆ°ng test cao hÆ¡n â†’ há»c thay vÃ¬ ghi nhá»›
- Test loss cá»§a Adam tÄƒng láº¡i sau epoch 45 (Ä‘iá»ƒn hÃ¬nh cá»§a overfit)
- Test loss cá»§a SAM giáº£m Ä‘á»u Ä‘áº·n suá»‘t 100 epochs

**Ã nghÄ©a:**
- **SAM vÃ´ cÃ¹ng giÃ¡ trá»‹ khi thiáº¿u data** - tÃ¬nh huá»‘ng ráº¥t phá»• biáº¿n trong thá»±c táº¿
- Giáº£m overfitting gáº§n má»™t ná»­a (37.9% â†’ 19.6%)
- Trong medical imaging, rare diseases - SAM cÃ³ thá»ƒ lÃ  game changer

---

#### So sÃ¡nh cross-experiment:

**Pattern chung:**
1. **SAM cáº£i thiá»‡n test accuracy trong Má»ŒI trÆ°á»ng há»£p** (100% success rate)
2. **Hiá»‡u quáº£ tá»· lá»‡ thuáº­n vá»›i Ä‘á»™ khÃ³:**
   - Easy (MNIST + simple model): +0.6-1.1%
   - Medium (CIFAR-10 + CNN): +2.5%
   - Hard (Small data / High LR): +9.5% / +22.5%

3. **SAM LUÃ”N giáº£m overfitting:**
   - Logistic: -29% gap
   - MLP: -80% gap
   - CNN: -37% gap
   - Small data: -48% gap

4. **Trade-off nháº¥t quÃ¡n:** 1.5-2x training time cho improvement

---

### 3. ÄÃ¡nh giÃ¡ vÃ  So sÃ¡nh

#### A. Hiá»‡u quáº£ cá»§a SAM

âœ… **Äiá»ƒm máº¡nh Ä‘Æ°á»£c chá»©ng minh:**

1. **Cáº£i thiá»‡n generalization consistently:**
   - Test accuracy tÄƒng trong 100% trÆ°á»ng há»£p
   - KhÃ´ng cÃ³ trÆ°á»ng há»£p nÃ o SAM kÃ©m hÆ¡n Adam
   - Improvement range: +0.6% Ä‘áº¿n +22.5%

2. **Chá»‘ng overfitting xuáº¥t sáº¯c:**
   - Overfitting gap giáº£m 29%-80% tÃ¹y thá»±c nghiá»‡m
   - Äáº·c biá»‡t hiá»‡u quáº£ khi data Ã­t (giáº£m 48%)
   - Train accuracy tháº¥p hÆ¡n nhÆ°ng test accuracy cao hÆ¡n

3. **á»”n Ä‘á»‹nh vÆ°á»£t trá»™i:**
   - Learning curves mÆ°á»£t mÃ  hÆ¡n Adam
   - Ãt spikes, Ã­t fluctuations
   - Maintain performance tá»‘t hÆ¡n vá» cuá»‘i training
   - Robust vá»›i hyperparameters (Ä‘áº·c biá»‡t learning rate)

4. **Scalability:**
   - Hoáº¡t Ä‘á»™ng tá»‘t tá»« model nhá» (8K params) Ä‘áº¿n lá»›n (600K+ params)
   - Tá»« dataset dá»… (MNIST) Ä‘áº¿n khÃ³ (CIFAR-10)
   - Káº¿t há»£p tá»‘t vá»›i: dropout, batch norm, data augmentation

âš ï¸ **Äiá»ƒm yáº¿u:**

1. **Chi phÃ­ tÃ­nh toÃ¡n:**
   - Training time tÄƒng 1.5-2x
   - Cáº§n 2 forward-backward passes má»—i iteration
   - Vá»›i model lá»›n/data nhiá»u, tá»•ng thá»i gian tÄƒng Ä‘Ã¡ng ká»ƒ

2. **Cáº£i thiá»‡n khÃ´ng Ä‘á»“ng Ä‘á»u:**
   - Standard setting: chá»‰ cáº£i thiá»‡n vá»«a pháº£i (+0.6-2.5%)
   - Cáº§n Ä‘iá»u kiá»‡n Ä‘áº·c biá»‡t Ä‘á»ƒ tháº¥y rÃµ giÃ¡ trá»‹ (+9-22%)
   - TrÃªn MNIST Ä‘Æ¡n giáº£n: benefit khÃ´ng ná»•i báº­t

3. **Hyperparameter tuning:**
   - Cáº§n chá»n rho phÃ¹ há»£p (0.05 lÃ  default tá»‘t)
   - Má»™t sá»‘ trÆ°á»ng há»£p cáº§n Ä‘iá»u chá»‰nh Ä‘á»ƒ Ä‘áº¡t optimal

#### B. So sÃ¡nh vá»›i Adam

| TiÃªu chÃ­ | Adam | SAM | ÄÃ¡nh giÃ¡ |
|----------|------|-----|----------|
| **Test Accuracy** | Baseline | +0.6% Ä‘áº¿n +22.5% | â­â­â­ SAM tháº¯ng |
| **Overfitting** | Cao hÆ¡n | Tháº¥p hÆ¡n 29-80% | â­â­â­ SAM tháº¯ng |
| **Training Speed** | 1x | 1.5-2x slower | â­â­â­ Adam tháº¯ng |
| **Stability** | Tá»‘t | Ráº¥t tá»‘t | â­â­ SAM tá»‘t hÆ¡n |
| **Robustness (LR)** | Nháº¡y cáº£m | Robust | â­â­â­ SAM tháº¯ng |
| **Small Data** | Overfit náº·ng | Generalize tá»‘t | â­â­â­ SAM tháº¯ng |
| **Implementation** | ÄÆ¡n giáº£n | ÄÆ¡n giáº£n | â­ Ngang nhau |
| **Memory Usage** | Baseline | ~TÆ°Æ¡ng Ä‘Æ°Æ¡ng | â­ Ngang nhau |

**Tá»•ng káº¿t:**
- **Performance:** SAM tháº¯ng Ã¡p Ä‘áº£o (5/8 categories)
- **Efficiency:** Adam tá»‘t hÆ¡n vá» speed
- SAM Ä‘Ã¡ng trade-off 2x time Ä‘á»ƒ láº¥y better accuracy + robustness

#### C. Khi nÃ o nÃªn dÃ¹ng SAM?

**âœ… NÃŠN dÃ¹ng SAM khi:**

1. **Accuracy lÃ  Æ°u tiÃªn sá»‘ 1:**
   - Production models cáº§n best possible performance
   - Competitions (Kaggle, etc.) - má»—i 0.1% Ä‘á»u quan trá»ng
   - High-stakes applications (medical, autonomous driving)

2. **Thiáº¿u dá»¯ liá»‡u training:**
   - Medical imaging: Ã­t labeled data
   - Rare diseases: small patient cohorts
   - Specialized domains: data scarce
   - â†’ SAM giáº£m overfit 48%, tÄƒng test acc 9.5%

3. **Gáº·p váº¥n Ä‘á» overfitting:**
   - Model lá»›n, data nhá»
   - Training loss giáº£m nhÆ°ng test loss tÄƒng
   - Train accuracy cao nhÆ°ng test accuracy tháº¥p
   - â†’ SAM giáº£m overfitting gap 37-80%

4. **Training khÃ´ng á»•n Ä‘á»‹nh:**
   - Loss spikes, divergence
   - KhÃ³ tune learning rate
   - Cáº§n robust training
   - â†’ SAM cho phÃ©p LR cao hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n

5. **Dataset khÃ³, model phá»©c táº¡p:**
   - CIFAR-10, ImageNet, custom datasets
   - ResNet, EfficientNet, Transformers
   - â†’ SAM tá»a sÃ¡ng trong Ä‘iá»u kiá»‡n challenging

6. **CÃ³ thá»i gian Ä‘á»ƒ train lÃ¢u:**
   - Research projects
   - Final model training
   - KhÃ´ng cáº§n real-time iteration

**âš ï¸ CÃ‚N NHáº®C dÃ¹ng Adam thÃ´ng thÆ°á»ng khi:**

1. **Prototyping nhanh:**
   - Cáº§n iterate nhiá»u experiments
   - Test architectures, hyperparameters
   - Speed > accuracy trong giai Ä‘oáº¡n nÃ y

2. **Dataset ráº¥t lá»›n, Ä‘Æ¡n giáº£n:**
   - Training time lÃ  bottleneck
   - Dataset dá»…, Ã­t overfit (vÃ­ dá»¥: well-augmented ImageNet)
   - Improvement cá»§a SAM khÃ´ng Ä‘Ã¡ng ká»ƒ so vá»›i thá»i gian tÄƒng

3. **TÃ i nguyÃªn háº¡n cháº¿:**
   - Limited GPU time
   - Need to train many models
   - Budget constraints

4. **Model Ä‘Æ¡n giáº£n:**
   - Logistic regression, shallow networks
   - SAM chá»‰ cáº£i thiá»‡n nháº¹ (~1%)
   - KhÃ´ng Ä‘Ã¡ng trade-off


## ğŸ”¬ Vá» SAM (Sharpness-Aware Minimization)

SAM lÃ  má»™t ká»¹ thuáº­t tá»‘i Æ°u giÃºp cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch:
- TÃ¬m cÃ¡c vÃ¹ng "pháº³ng" trong khÃ´ng gian tham sá»‘ (flat minima)
- Thá»±c hiá»‡n 2 láº§n forward-backward pass má»—i iteration
- Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p test mÃ  khÃ´ng overfitting

**Trade-off**: Thá»i gian huáº¥n luyá»‡n tÄƒng gáº¥p ~2 láº§n so vá»›i Adam thÃ´ng thÆ°á»ng.

---

## ğŸ’¡ Tips quan trá»ng

1. **GPU**: 
   - **Báº®T BUá»˜C** cÃ i Ä‘áº·t PyTorch vá»›i CUDA support náº¿u cÃ³ GPU NVIDIA
   - Kiá»ƒm tra báº±ng `nvidia-smi` vÃ  `python check_gpu.py`
   - Thá»i gian cháº¡y nhanh hÆ¡n 10-50x so vá»›i CPU
2. **Virtual Environment**: 
   - Náº¿u dÃ¹ng venv, nhá»› kÃ­ch hoáº¡t trÆ°á»›c khi cháº¡y
3. **Data**: Dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng táº£i xuá»‘ng vÃ o thÆ° má»¥c `./data`
4. **Reproducibility**: ÄÃ£ set seed=42 cho táº¥t cáº£ cÃ¡c thá»±c nghiá»‡m
5. **Memory**: CNN trÃªn CIFAR-10 cáº§n nhiá»u RAM/VRAM nháº¥t (~2-4GB VRAM)

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Sharpness-Aware Minimization Paper (Foret et al., 2020)](https://arxiv.org/abs/2010.01412)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)
- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)

---

## ğŸ’¬ ÄÃ³ng gÃ³p

Dá»± Ã¡n nÃ y má»Ÿ cho má»i Ä‘Ã³ng gÃ³p! Náº¿u báº¡n muá»‘n:
- ThÃªm thá»±c nghiá»‡m má»›i
- Cáº£i thiá»‡n code
- Fix bugs
- Viáº¿t documentation
- Chia sáº» insights

HÃ£y má»Ÿ Issue hoáº·c Pull Request trÃªn GitHub!

---

**ChÃºc báº¡n thá»±c nghiá»‡m thÃ nh cÃ´ng! ğŸ‰**

*"Flat minima generalize better than sharp minima" - A journey through SAM*
