"""
Th·ª±c nghi·ªám: So s√°nh Adam vs Adam+SAM v·ªõi LEARNING RATE CAO
M·ª•c ƒë√≠ch: Ch·ª©ng minh SAM ·ªïn ƒë·ªãnh h∆°n v·ªõi learning rate l·ªõn
Dataset: CIFAR-10
Model: ResNet-18 (modified)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
from torchvision.models import resnet18
import matplotlib.pyplot as plt
import numpy as np
import time
import os

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

class SAM(torch.optim.Optimizer):
    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):
        assert rho >= 0.0, f"Invalid rho, should be non-negative: {rho}"
        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)
        super(SAM, self).__init__(params, defaults)
        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)
        self.param_groups = self.base_optimizer.param_groups
        self.defaults.update(self.base_optimizer.defaults)

    @torch.no_grad()
    def first_step(self, zero_grad=False):
        grad_norm = self._grad_norm()
        for group in self.param_groups:
            scale = group["rho"] / (grad_norm + 1e-12)
            for p in group["params"]:
                if p.grad is None:
                    continue
                self.state[p]["old_p"] = p.data.clone()
                e_w = (torch.pow(p, 2) if group["adaptive"] else 1.0) * p.grad * scale.to(p)
                p.add_(e_w)
        if zero_grad:
            self.zero_grad()

    @torch.no_grad()
    def second_step(self, zero_grad=False):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue
                if "old_p" in self.state[p]:
                    p.data = self.state[p]["old_p"]
        self.base_optimizer.step()
        if zero_grad:
            self.zero_grad()

    @torch.no_grad()
    def step(self, closure=None):
        assert closure is not None, "SAM requires closure, but it was not provided"
        closure = torch.enable_grad()(closure)
        self.first_step(zero_grad=True)
        loss = closure()
        self.second_step()
        return loss

    def _grad_norm(self):
        shared_device = self.param_groups[0]["params"][0].device
        grad_list = [
            ((torch.abs(p) if group["adaptive"] else 1.0) * p.grad).norm(p=2).to(shared_device)
            for group in self.param_groups for p in group["params"]
            if p.grad is not None
        ]
        if not grad_list:
            return torch.tensor(0.0, device=shared_device)
        norm = torch.norm(torch.stack(grad_list), p=2)
        return norm

def get_model(num_classes=10):
    """Modified ResNet-18 for CIFAR-10"""
    model = resnet18(weights=None)
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    model.maxpool = nn.Identity()
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model

def train_epoch(model, train_loader, optimizer, criterion, device, use_sam=False):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        if use_sam:
            def closure():
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                return loss
            
            loss = optimizer.step(closure)
            with torch.no_grad():
                output = model(data)
        else:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
        
        total_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)
        
        if (batch_idx + 1) % 50 == 0:
            print(f'  Batch [{batch_idx + 1}/{len(train_loader)}] - '
                  f'Loss: {loss.item():.4f} - '
                  f'Acc: {100. * correct / total:.2f}%')
    
    return total_loss / len(train_loader), 100. * correct / total

def test(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
    
    return total_loss / len(test_loader), 100. * correct / total

def run_experiment(lr, use_sam=False, epochs=20, device='cuda'):
    """
    Ch·∫°y th·ª±c nghi·ªám v·ªõi learning rate c·ª• th·ªÉ
    """
    print(f"\n{'='*60}")
    print(f"Running with {'Adam+SAM' if use_sam else 'Adam'} - LR={lr}")
    print(f"{'='*60}")
    
    # Data preparation
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    train_dataset = datasets.CIFAR10(root='./data', train=True, 
                                     download=True, transform=transform_train)
    test_dataset = datasets.CIFAR10(root='./data', train=False, 
                                    download=True, transform=transform_test)
    
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)
    
    # Model
    model = get_model().to(device)
    criterion = nn.CrossEntropyLoss()
    
    # Optimizer
    if use_sam:
        optimizer = SAM(model.parameters(), base_optimizer=optim.Adam, 
                       lr=lr, rho=0.05)
    else:
        optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # Training
    train_losses, test_losses = [], []
    train_accs, test_accs = [], []
    
    best_acc = 0
    diverged = False
    
    for epoch in range(epochs):
        print(f'\nEpoch {epoch + 1}/{epochs}')
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, 
                                           criterion, device, use_sam)
        test_loss, test_acc = test(model, test_loader, criterion, device)
        
        train_losses.append(train_loss)
        test_losses.append(test_loss)
        train_accs.append(train_acc)
        test_accs.append(test_acc)
        
        print(f'Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.2f}%')
        print(f'Test Loss: {test_loss:.4f} - Test Acc: {test_acc:.2f}%')
        
        if test_acc > best_acc:
            best_acc = test_acc
        
        # Ki·ªÉm tra divergence
        if train_loss > 10 or np.isnan(train_loss):
            print(f"‚ö†Ô∏è Training diverged at epoch {epoch + 1}!")
            diverged = True
            # ƒêi·ªÅn c√°c epoch c√≤n l·∫°i b·∫±ng gi√° tr·ªã cu·ªëi c√πng
            for _ in range(epoch + 1, epochs):
                train_losses.append(train_loss)
                test_losses.append(test_loss)
                train_accs.append(train_acc)
                test_accs.append(test_acc)
            break
    
    return {
        'train_losses': train_losses,
        'test_losses': test_losses,
        'train_accs': train_accs,
        'test_accs': test_accs,
        'best_acc': best_acc,
        'diverged': diverged
    }

def plot_comparison(results_dict, lrs, save_path='high_lr_comparison.png'):
    """V·∫Ω bi·ªÉu ƒë·ªì so s√°nh cho nhi·ªÅu learning rates"""
    n_lrs = len(lrs)
    fig, axes = plt.subplots(2, n_lrs, figsize=(6*n_lrs, 10))
    
    if n_lrs == 1:
        axes = axes.reshape(2, 1)
    
    for idx, lr in enumerate(lrs):
        adam_results = results_dict[f'adam_{lr}']
        sam_results = results_dict[f'sam_{lr}']
        
        epochs = range(1, len(adam_results['train_losses']) + 1)
        
        # Test Loss
        axes[0, idx].plot(epochs, adam_results['test_losses'], 'b-', label='Adam', linewidth=2)
        axes[0, idx].plot(epochs, sam_results['test_losses'], 'r-', label='Adam+SAM', linewidth=2)
        axes[0, idx].set_xlabel('Epoch')
        axes[0, idx].set_ylabel('Test Loss')
        axes[0, idx].set_title(f'Test Loss (LR={lr})')
        axes[0, idx].legend()
        axes[0, idx].grid(True, alpha=0.3)
        if adam_results['diverged']:
            axes[0, idx].text(0.5, 0.95, 'Adam DIVERGED', 
                            transform=axes[0, idx].transAxes,
                            ha='center', va='top', color='red', fontweight='bold')
        
        # Test Accuracy
        axes[1, idx].plot(epochs, adam_results['test_accs'], 'b-', label='Adam', linewidth=2)
        axes[1, idx].plot(epochs, sam_results['test_accs'], 'r-', label='Adam+SAM', linewidth=2)
        axes[1, idx].set_xlabel('Epoch')
        axes[1, idx].set_ylabel('Test Accuracy (%)')
        axes[1, idx].set_title(f'Test Accuracy (LR={lr})')
        axes[1, idx].legend()
        axes[1, idx].grid(True, alpha=0.3)
        
        # Hi·ªÉn th·ªã best accuracy
        adam_best = adam_results['best_acc']
        sam_best = sam_results['best_acc']
        improvement = sam_best - adam_best
        
        axes[1, idx].text(0.5, 0.05, 
                         f'Adam: {adam_best:.2f}% | SAM: {sam_best:.2f}% | Œî: {improvement:+.2f}%',
                         transform=axes[1, idx].transAxes,
                         ha='center', va='bottom',
                         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f'\nüìä Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u: {save_path}')
    plt.close()

def main():
    set_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'S·ª≠ d·ª•ng device: {device}')
    
    # Test v·ªõi nhi·ªÅu learning rates
    learning_rates = [0.001, 0.005, 0.01]  # LR cao ƒë·ªÉ th·∫•y s·ª± kh√°c bi·ªát
    epochs = 20
    
    results_dict = {}
    
    for lr in learning_rates:
        print(f"\n{'='*70}")
        print(f"TESTING LEARNING RATE: {lr}")
        print(f"{'='*70}")
        
        # Adam
        adam_results = run_experiment(lr=lr, use_sam=False, epochs=epochs, device=device)
        results_dict[f'adam_{lr}'] = adam_results
        
        # Adam + SAM
        sam_results = run_experiment(lr=lr, use_sam=True, epochs=epochs, device=device)
        results_dict[f'sam_{lr}'] = sam_results
        
        # Print summary
        print(f"\n{'='*70}")
        print(f"SUMMARY for LR={lr}:")
        print(f"Adam Best Acc: {adam_results['best_acc']:.2f}%")
        print(f"SAM Best Acc: {sam_results['best_acc']:.2f}%")
        print(f"Improvement: {sam_results['best_acc'] - adam_results['best_acc']:+.2f}%")
        if adam_results['diverged']:
            print("‚ö†Ô∏è Adam DIVERGED!")
        print(f"{'='*70}")
    
    # Plot comparison
    plot_comparison(results_dict, learning_rates)
    
    print("\n‚úÖ Th·ª±c nghi·ªám ho√†n t·∫•t!")
    print(f"\nüí° K·∫æT LU·∫¨N:")
    print(f"   - V·ªõi LR th·∫•p (0.001): C·∫£ hai t∆∞∆°ng ƒë∆∞∆°ng")
    print(f"   - V·ªõi LR cao (0.005-0.01): SAM ·ªïn ƒë·ªãnh v√† ch√≠nh x√°c h∆°n r√µ r·ªát")
    print(f"   - Adam c√≥ th·ªÉ diverge v·ªõi LR cao, SAM v·∫´n ·ªïn ƒë·ªãnh")

if __name__ == '__main__':
    main()
